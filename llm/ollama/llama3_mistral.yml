settings:
  name: "Ollama with parametrized input to run on Llama3 and Mistral"
  image: ollama/ollama
  cpu: 16384
  memory: 122880
  timeout: 300
  example: satori run satori://llm/ollama/llama3_mistral.yml -d MODEL="model" -d INPUT="input" --output

MODEL:
- - "llama3"
  - "mistral"

install:
  dependencies:
  - apt update >> /dev/null
  - apt install -qy screen jq curl >> /dev/null

ollama:
  serve:
  - screen -dm ollama serve; sleep 1
  pull:
  - ollama pull ${{MODEL}}
  generate:
  - "curl -s http://localhost:11434/api/generate -d '{\"model\": \"${{MODEL}}\",\"prompt\":\"${{INPUT}}\"}' | grep \"model.*response\" | jq -r 'select(.done==false) | .response' | tr -d '\\n'"
